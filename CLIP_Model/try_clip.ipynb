{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43c7acdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HCMUS_ComputerScience\\ComputationalThinking\\TeamProject\\reactProject\\CLIP_Model\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load a pre-trained CLIP model\n",
    "model = SentenceTransformer('clip-ViT-B-32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84811073",
   "metadata": {},
   "outputs": [],
   "source": [
    "## declare constant and necessary variables\n",
    "DB_path = \"../DataCollector/result/places.db\"\n",
    "DB_name = \"places\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "368286b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sqlite3\n",
    "from typing import List\n",
    "import torch\n",
    "\n",
    "def parseList(image_list):\n",
    "    print(type(image_list))\n",
    "    return json.loads(image_list)\n",
    "\n",
    "def getImages(id):\n",
    "    \"Return a list of PIL images\"\n",
    "    conn = sqlite3.connect(DB_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"SELECT images FROM {DB_name} WHERE rowid = ?\", (id,))\n",
    "    image_url = cursor.fetchone()[0]\n",
    "    image_url = parseList(image_url)\n",
    "    pil_image = []\n",
    "    for url in image_url:\n",
    "        print(url)\n",
    "        pil_image.append(Image.open(requests.get(url, stream=True).raw))\n",
    "\n",
    "    return pil_image\n",
    "\n",
    "def embedImageList(images):\n",
    "    if len(images) == 0:\n",
    "        return torch.zeros(1, 512)\n",
    "    print(\"len(images) = \",len(images))\n",
    "    encoding = [model.encode(image) for image in images]\n",
    "    print(type(encoding[0]))\n",
    "    # list of numpy.ndarray\n",
    "    return sum(encoding) / len(encoding)\n",
    "    \n",
    "\n",
    "def embedText(text):\n",
    "    return model.encode(text)\n",
    "\n",
    "def getSimilarity(imageList, text):\n",
    "    image_emb = embedImageList(imageList)\n",
    "    text_emb = embedText(text)\n",
    "    return util.cos_sim(image_emb, text_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38b97494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "https://lh3.googleusercontent.com/p/AF1QipOSisTfpcYNbHV-0X3CB0E7pedEkh10avYId7eI=s0\n",
      "https://lh3.googleusercontent.com/geougc-cs/AMBA38t2Kmr5_XJSv2PmDO10nabcToVMxqXZwW_M3PM9su_8PO1ED7uHBVqxnIGzYii6adp4DKOc07hYToYMGDCTK0A3sAnVZ0OcbUO7mh2w_5YBiC8MdqHRYBeFKFKBpXXethGqvS89=s0\n",
      "https://lh3.googleusercontent.com/geougc-cs/AMBA38v8qd5Fo9KkbRzRKrYOOmP0JZxnFWPrjFUB2mbsDD04wT1pUBB01TM531RbG3_ZpfpUGoJgt2bvWrR0k5fFq7jkd3sp2m60IGig0JZJXf_wFPc2ANdfgFi1GH8-L_Wlg3dm-r9TpA=s0\n",
      "[[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1080x1440 at 0x288D4B52910>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=3024x4032 at 0x288D4B3CB50>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4032x3024 at 0x288D4A0E710>]]\n",
      "len(images) =  3\n",
      "<class 'numpy.ndarray'>\n",
      "[tensor([[0.2461]])]\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Đồ ăn ngon\"\n",
    "sample_images = [getImages(i) for i in range(2, 3)]\n",
    "\n",
    "print(sample_images)\n",
    "\n",
    "\n",
    "# my_embeddings = model.encode(sample_text)\n",
    "print([getSimilarity(image, sample_text) for image in sample_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a8e91e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index loaded successfully from test_images.bin\n",
      "Index type: <class 'faiss.swigfaiss_avx2.IndexIDMap2'>\n",
      "Total number of vectors in the index: 2875\n",
      "Missing IDs: {1852}\n",
      "Missing IDs: 1\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "# Define the path to your .bin file\n",
    "index_file_path = \"test_images.bin\"\n",
    "\n",
    "# Load the index from the file\n",
    "try:\n",
    "    index = faiss.read_index(index_file_path)\n",
    "    print(f\"Index loaded successfully from {index_file_path}\")\n",
    "    print(f\"Index type: {type(index)}\")\n",
    "    print(f\"Total number of vectors in the index: {index.ntotal}\")\n",
    "    ## print missing id from 1 to 2900\n",
    "\n",
    "    ids_np = faiss.vector_to_array(index.id_map)\n",
    "    ids_in_index = set(ids_np.tolist())\n",
    "\n",
    "    expected_ids = set(range(1, 2876))\n",
    "    missing = expected_ids - ids_in_index\n",
    "\n",
    "    print(\"Missing IDs:\", missing)\n",
    "\n",
    "    print(\"Missing IDs:\", len(missing))\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error loading index: {e}\")\n",
    "    print(\"Ensure the file path is correct and the file is a valid FAISS index file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8441856d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1852, 'https://lh3.googleusercontent.com/gps-cs-s/AG0ilSzFnBa-B0UbGoTTsoChbLAtqHnBzOPgVTFz_H8CjThSplPNn2_sARxmTvt8gXQdt-9FDrBmRFZ3NCbVuV5nvLGCLeQI8-xNTMZs3W2_ITxyzzrkKnuN_YfrgO-VxwThLxCR0nJ57mVMuUk=s0')]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "## add missing ids to index\n",
    "\n",
    "conn = sqlite3.connect(\"../DataCollector/result/images_embedding.db\")\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(f\"SELECT rowid, url FROM test_images WHERE rowid IN ({', '.join(map(str, missing))})\")\n",
    "results = cursor.fetchall()\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2dc1482",
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop faiss index at 1852\n",
    "import numpy as np\n",
    "import faiss\n",
    "index = faiss.read_index(\"test_images.bin\")\n",
    "index.remove_ids(np.array([1852], dtype=np.int64))\n",
    "faiss.write_index(index, \"test_images.bin\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c88f9d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1852 https://lh3.googleusercontent.com/gps-cs-s/AG0ilSzFnBa-B0UbGoTTsoChbLAtqHnBzOPgVTFz_H8CjThSplPNn2_sARxmTvt8gXQdt-9FDrBmRFZ3NCbVuV5nvLGCLeQI8-xNTMZs3W2_ITxyzzrkKnuN_YfrgO-VxwThLxCR0nJ57mVMuUk=s0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "result = results[0]\n",
    "## open faiss index again\n",
    "index = faiss.read_index(\"test_images.bin\")\n",
    "id, url = result\n",
    "print(id, url)\n",
    "resp = requests.get(url, timeout=10)\n",
    "resp.raise_for_status()\n",
    "\n",
    "img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "embedding = model.encode(img, normalize_embeddings=True, convert_to_numpy=True).astype(np.float32)\n",
    "index.add_with_ids(embedding.reshape(1, -1), np.array([id], dtype=np.int64))\n",
    "\n",
    "faiss.write_index(index, \"test_images.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75a1c2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2949]])\n",
      "tensor([[0.2328]])\n",
      "tensor([[0.2362]])\n",
      "tensor([[0.2016]])\n",
      "tensor([[0.2157]])\n",
      "tensor([[0.1786]])\n",
      "tensor([[0.2072]])\n"
     ]
    }
   ],
   "source": [
    "food = \"https://lh3.googleusercontent.com/geougc-cs/AMBA38tjkOByp4DU0-4-YrmbeXz3sHpteXD6IelGuoMxZfNOLg2TJVLoqHnWMEhBrHeb3KdQkt2InycNnnijlDYAi6X-N3UqykUm1rHMf0Xsp1rNuKYlNnGqaNgfqKZgx_k2jMpmyeIY=s0\"\n",
    "restaurant = \"https://lh3.googleusercontent.com/gps-cs-s/AG0ilSzMtPXeE1uEkq2fRV1rpRaBW5MZMN-caZKmFtpRkjdvwC2LdEHP6xHOcwxsBEr2mObNWrCh5pFkq9InYJFr50TAUlfVGJS1P62hnKukoXOCmCglpwUz3skQhOcDLKx5H-JsW3Gq=s0\"\n",
    "mrInmenu = \"https://lh3.googleusercontent.com/geougc-cs/AMBA38vz3isPKRASPEXTJDq8sgL9Ve0jp4SepgQhw6lWUY6CnwOqC8hFjuqPY6eFtG0u2gJZvxRBM46HpxRisrhhtSulyqVrxiBvFbbVO8bdR_XJ4lAeA7d90vdpailpeUfrQkMzd-R8yA=s0\"\n",
    "phomenu = \"https://lh3.googleusercontent.com/geougc-cs/AMBA38v1aQd-mplhpMrPgxU5-YjF3bk_OYHpU60lXEdJ-vnLHSOpENZwiTfYQr2kDboZIDTe2roVrObhLtWQtDnTatAXuN-pTxMNIs3RSo9HMx13qJQWyvfge1lXzUop93gTpChGpVrBZA=s0\"\n",
    "phoimage = \"https://lh3.googleusercontent.com/gps-cs-s/AG0ilSwJ-cwjeEWaqQx33AkZLrVGk3SRQ8WmijDHXKn7elBvB6lATdCQvzJhLJUOw5CPP2oIMvUzMXOlVGJ6HmZpl8O_2K0nNmm9Vz_Z0sd1qwEwdWbUlPF6Wgz73F16cJOBEr39uOFn=s0\"\n",
    "dauhurangmuoipho = \"https://lh3.googleusercontent.com/geougc-cs/AMBA38uMoMAaMREZj9SFUMWOPZtUqn_7CnziB2vRK2N5yMed0jon3twSykZDevZfkGa73GHGlPa5-HAoUbSWMDFs37iIvN6EV8hu0K9HusUxLbZ6z5F9GDVPVYLLCBrvma_pQMRgHi-t9Q=s0\"\n",
    "banhtrangtron = \"https://lh3.googleusercontent.com/geougc-cs/AMBA38t0abjIRTf0wDT7zlKsr1lFddm20spRsVIP1jJ4EtQFNb51JBWzz1KZj_2h3gWmq0e72_viEDGlhBpqGf8Foi4ScQ6ZSAbPsFlNe45WJfsglZJkGzinXsHEPic-sb9_BbAsz05L=s0\"\n",
    "\n",
    "\n",
    "text = \"Japanese food\"\n",
    "\n",
    "def getSimilarity(url, text):\n",
    "    # model = SentenceTransformer('clip-ViT-B-32')\n",
    "    image = Image.open(requests.get(url, stream=True).raw)\n",
    "    embedding = model.encode(image, normalize_embeddings=True, convert_to_numpy=True)\n",
    "    text_embedding = model.encode(text, normalize_embeddings=True, convert_to_numpy=True)\n",
    "    sim = util.cos_sim(embedding, text_embedding)\n",
    "    return sim\n",
    "\n",
    "print(getSimilarity(food, text))\n",
    "print(getSimilarity(restaurant, text))\n",
    "print(getSimilarity(phoimage, text))\n",
    "print(getSimilarity(mrInmenu, text))\n",
    "print(getSimilarity(dauhurangmuoipho, text))\n",
    "# print(getSimilarity(souvenir, text))\n",
    "print(getSimilarity(banhtrangtron, text))\n",
    "print(getSimilarity(phomenu, text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac275954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2823]])\n"
     ]
    }
   ],
   "source": [
    "doannhatban = \"https://lh3.googleusercontent.com/geougc-cs/AMBA38vGIgxWXqEhkjMJeiHGHDv7DZkXGnZNfjt0cGUaBEEpuV3kbLC9s6AwjjsThe09R0gwa3LQn6y6uRmpouFn5Y3va4GU5UTJdQdDJ6IukWoKO5Jy4HMjIjETBoJHNGmXBJRH915t=s0\"\n",
    "print(getSimilarity(doannhatban, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19bc0a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
