{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7911e838",
   "metadata": {},
   "outputs": [],
   "source": [
    "## try NER\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "193e4327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "doc = nlp(\"Located in a prime location on Le Duan Street, District 1, Diamond Department Store (Diamond Plaza) is a symbol of luxury shopping right in the heart of Saigon. It houses numerous high-end fashion, cosmetics, jewelry, and homeware brands from all over the world. Not just an ideal destination for shopping enthusiasts, Diamond Plaza also offers diverse culinary experiences and modern entertainment spaces, making it a perfect stop for a day of exploring Saigon with style and convenience.\")\n",
    "take_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "badede94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.05368375778198242 seconds\n",
      "Located VERB \n",
      "in ADP \n",
      "a DET \n",
      "prime ADJ \n",
      "location NOUN \n",
      "on ADP \n",
      "Le PROPN FAC\n",
      "Duan PROPN FAC\n",
      "Street PROPN FAC\n",
      ", PUNCT \n",
      "District PROPN DATE\n",
      "1 NUM DATE\n",
      ", PUNCT \n",
      "Diamond PROPN ORG\n",
      "Department PROPN ORG\n",
      "Store PROPN \n",
      "( PUNCT \n",
      "Diamond PROPN GPE\n",
      "Plaza PROPN GPE\n",
      ") PUNCT \n",
      "is AUX \n",
      "a DET \n",
      "symbol NOUN \n",
      "of ADP \n",
      "luxury NOUN \n",
      "shopping NOUN \n",
      "right ADV \n",
      "in ADP \n",
      "the DET \n",
      "heart NOUN \n",
      "of ADP \n",
      "Saigon PROPN GPE\n",
      ". PUNCT \n",
      "It PRON \n",
      "houses VERB \n",
      "numerous ADJ \n",
      "high ADJ \n",
      "- PUNCT \n",
      "end NOUN \n",
      "fashion NOUN \n",
      ", PUNCT \n",
      "cosmetics NOUN \n",
      ", PUNCT \n",
      "jewelry NOUN \n",
      ", PUNCT \n",
      "and CCONJ \n",
      "homeware VERB \n",
      "brands NOUN \n",
      "from ADP \n",
      "all ADV \n",
      "over ADP \n",
      "the DET \n",
      "world NOUN \n",
      ". PUNCT \n",
      "Not PART \n",
      "just ADV \n",
      "an DET \n",
      "ideal ADJ \n",
      "destination NOUN \n",
      "for ADP \n",
      "shopping NOUN \n",
      "enthusiasts NOUN \n",
      ", PUNCT \n",
      "Diamond PROPN ORG\n",
      "Plaza PROPN ORG\n",
      "also ADV \n",
      "offers VERB \n",
      "diverse ADJ \n",
      "culinary ADJ \n",
      "experiences NOUN \n",
      "and CCONJ \n",
      "modern ADJ \n",
      "entertainment NOUN \n",
      "spaces NOUN \n",
      ", PUNCT \n",
      "making VERB \n",
      "it PRON \n",
      "a DET \n",
      "perfect ADJ \n",
      "stop NOUN \n",
      "for ADP \n",
      "a DET DATE\n",
      "day NOUN DATE\n",
      "of ADP \n",
      "exploring VERB \n",
      "Saigon PROPN GPE\n",
      "with ADP \n",
      "style NOUN \n",
      "and CCONJ \n",
      "convenience NOUN \n",
      ". PUNCT \n"
     ]
    }
   ],
   "source": [
    "print(f\"Time taken: {take_time} seconds\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4798a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HCMUS_ComputerScience\\ComputationalThinking\\TeamProject\\reactProject\\CLIP_Model\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\HCMUS_ComputerScience\\ComputationalThinking\\TeamProject\\reactProject\\CLIP_Model\\venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\PC\\.cache\\huggingface\\hub\\models--dslim--bert-base-NER. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-PER', 'score': np.float32(0.9990139), 'index': 4, 'word': 'Wolfgang', 'start': 11, 'end': 19}, {'entity': 'B-LOC', 'score': np.float32(0.999645), 'index': 9, 'word': 'Berlin', 'start': 34, 'end': 40}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "example = \"My name is Wolfgang and I live in Berlin\"\n",
    "\n",
    "ner_results = nlp(example)\n",
    "print(ner_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8339ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def heuristic_split(text):\n",
    "    \"\"\"\n",
    "    Split text into sentences, avoiding splits inside numbers, abbreviations, etc.\n",
    "    \"\"\"\n",
    "    # Bước 1: Replace dấu chấm trong các trường hợp đặc biệt bằng token tạm\n",
    "    # Ví dụ: số kiểu 123.456, IP, version\n",
    "    text = re.sub(r'(?<=\\d)\\.(?=\\d)', '<DOT>', text)\n",
    "    \n",
    "    # Có thể thêm các abbreviations phổ biến: Mr., Dr., etc.\n",
    "    abbreviations = ['Mr', 'Mrs', 'Dr', 'Ms', 'Prof']\n",
    "    for abbr in abbreviations:\n",
    "        text = re.sub(rf'\\b{abbr}\\.(?=\\s)', f'{abbr}<DOT>', text)\n",
    "    \n",
    "    # Bước 2: Split theo dấu chấm / ! / ? thực sự\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    \n",
    "    # Bước 3: Thay token <DOT> về lại dấu chấm\n",
    "    sentences = [s.replace('<DOT>', '.') for s in sentences]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ad6e8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## read from ../DataCollector/result/places.db\n",
    "## read rowid and desciption (is a string)\n",
    "## split string by '.'\n",
    "## create new db with (rowid, descrption in sentence)\n",
    "## each sentence is a row\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('../../DataCollector/result/places.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"SELECT rowid, description FROM places\")\n",
    "descs = cursor.fetchall()\n",
    "\n",
    "list_add_to_new_db = []\n",
    "for id_desc in descs:\n",
    "    id, desc = id_desc\n",
    "    desc = heuristic_split(desc)\n",
    "    ## pop all '' string\n",
    "    desc = [sentence for sentence in desc if sentence != '']\n",
    "    for sentence in desc:\n",
    "        list_add_to_new_db.append((id, sentence))\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "771dea1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1382\n"
     ]
    }
   ],
   "source": [
    "print(len(list_add_to_new_db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72a360b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created successfully\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('places_sentences.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"CREATE TABLE if not exists places_sentences (place_id INTEGER, sentence TEXT)\")\n",
    "for id_sentence in list_add_to_new_db:\n",
    "    cursor.execute(\"INSERT INTO places_sentences (place_id, sentence) VALUES (?, ?)\", id_sentence)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"Database created successfully\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e43aa602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HCMUS_ComputerScience\\ComputationalThinking\\TeamProject\\reactProject\\CLIP_Model\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load model\n",
    "model_name = \"paraphrase-mpnet-base-v2\"\n",
    "model = SentenceTransformer(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a10da8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 2, 2, 2, 3, 3]\n",
      "['agataJapan at 19 Đặng Dung, District 1 is an ideal destination for those who love Japanese culture and products.', 'Here, you will discover a sophisticated miniature world of the Land of Cherry Blossoms, from charming decor items, meticulous handicrafts, to practical products and fashion pieces imbued with distinct Japanese style.', 'Each product is carefully curated, ensuring quality and uniqueness.', 'agataJapan is not just a store, but also an elegant, peaceful experiential space amidst the vibrant heart of Saigon.', 'Come visit to immerse yourself and bring a touch of Japan home with you!', 'TOÀN UYÊN FOOD – The ideal destination for quality food!', 'Specializing in wholesale & retail of various delicious skewers, convenient frozen food, and fresh seafood.', \"We ensure diverse and safe ingredients for all your family's culinary needs or your business, located at 3/12 De Tham Street, Co Giang Ward, District 1.\", 'Welcome to My Vi Eatery at 35 Ky Con, District 1 – where the essence of culinary art converges right in the heart of bustling Saigon!', 'True to its name, each dish here offers an unforgettable taste experience, meticulously prepared with rich, authentic flavors.']\n"
     ]
    }
   ],
   "source": [
    "## select all sentence from places_sentences sort by rowid\n",
    "conn = sqlite3.connect('places_sentences.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT place_id, sentence FROM places_sentences\")\n",
    "result = cursor.fetchall()\n",
    "\n",
    "conn.close()\n",
    "\n",
    "index = [a[0] for a in result]\n",
    "sentences = [a[1] for a in result]\n",
    "print(index[:10])\n",
    "print(sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23db2034",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/44 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 44/44 [01:05<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "embeddings = model.encode(sentences, convert_to_tensor=True, batch_size=32, show_progress_bar=True)\n",
    "embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)\n",
    "embeddings_np = embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fb50f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "index = np.array(index, dtype=np.int32)\n",
    "arrays = {\n",
    "    'index': index,\n",
    "    'embeddings': embeddings_np\n",
    "}\n",
    "\n",
    "with open('arrays.pkl', 'wb') as f:\n",
    "    pickle.dump(arrays, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a7d3e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 2 2 2 3 3]\n",
      "(1834, 768)\n"
     ]
    }
   ],
   "source": [
    "with open('arrays.pkl', 'rb') as f:\n",
    "    arrays = pickle.load(f)\n",
    "\n",
    "index = arrays['index']\n",
    "embeddings = arrays['embeddings']\n",
    "\n",
    "print(index[:10])\n",
    "print(embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7c6d980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1834, 1)\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Place to enjoy cultural banh mi in HCM\"\n",
    "\n",
    "query_embedding = model.encode([sample_text], convert_to_tensor=True)\n",
    "query_embedding = torch.nn.functional.normalize(query_embedding, p=2, dim=1)\n",
    "query_embedding_np = query_embedding.cpu().numpy()\n",
    "\n",
    "# Tính toán cosine similarity giữa query embedding và tất cả embeddings\n",
    "cosine_similarities = np.dot(embeddings, query_embedding_np.T)\n",
    "print(cosine_similarities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8a39b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1191\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(cosine_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ecc78dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1834, 2)\n",
      "[ 452  607  410 1024  653  964  355 1234  912 1191]\n"
     ]
    }
   ],
   "source": [
    "## find out top k index with highest cosine similarity\n",
    "k = 10\n",
    "concate_id_and_similarity = np.concatenate((index.reshape(-1, 1), cosine_similarities), axis=1)\n",
    "print(concate_id_and_similarity.shape)\n",
    "top_k_indices = np.argsort(concate_id_and_similarity, axis=0)[-k:]\n",
    "ans = top_k_indices[:, 1]\n",
    "print(ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0922d549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6558473 ]\n",
      " [0.6569691 ]\n",
      " [0.65774906]\n",
      " [0.658208  ]\n",
      " [0.6693357 ]\n",
      " [0.6741901 ]\n",
      " [0.6888634 ]\n",
      " [0.71064216]\n",
      " [0.72482985]\n",
      " [0.7258788 ]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarities[ans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cbe3352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1824  452]\n",
      " [1825  607]\n",
      " [1826  410]\n",
      " [1827 1024]\n",
      " [1828  653]\n",
      " [1829  964]\n",
      " [1830  355]\n",
      " [1831 1234]\n",
      " [1832  912]\n",
      " [1833 1191]]\n"
     ]
    }
   ],
   "source": [
    "print(top_k_indices[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd40aaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('',), ('',), ('',), ('Saigon vegetarian banh mi offers an irresistibly delicious flavor with a crispy baguette crust, a light and wholesome filling packed with fresh vegetables, tofu, and mushrooms, all richly seasoned',), ('',), ('',), (' You\\'ll definitely be hopelessly \"addicted\"!',), (' An unforgettable culinary experience, enticing with every spoonful!',), ('\"Bánh mì góc hẻm\" – the name says it all about its rustic charm and the distinctive flavor of Saigon',), (' Savor a crispy, piping hot banh mi sandwich generously filled with fragrant pate, flavorful cold cuts, rich Vietnamese pork sausage, fresh herbs, and a touch of spicy chili',)]\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "## query ans in places_sentences and print\n",
    "ans_as_string = [str(a) for a in ans]\n",
    "conn = sqlite3.connect('places_sentences.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT sentence FROM places_sentences WHERE rowid IN ({})\".format(','.join(ans_as_string)))\n",
    "result = cursor.fetchall()\n",
    "conn.close()\n",
    "\n",
    "print(result)\n",
    "print(len(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ab3a4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Saigon vegetarian banh mi offers an irresistibly delicious flavor with a crispy baguette crust, a light and wholesome filling packed with fresh vegetables, tofu, and mushrooms, all richly seasoned\n",
      "\n",
      "\n",
      " You'll definitely be hopelessly \"addicted\"!\n",
      " An unforgettable culinary experience, enticing with every spoonful!\n",
      "\"Bánh mì góc hẻm\" – the name says it all about its rustic charm and the distinctive flavor of Saigon\n",
      " Savor a crispy, piping hot banh mi sandwich generously filled with fragrant pate, flavorful cold cuts, rich Vietnamese pork sausage, fresh herbs, and a touch of spicy chili\n"
     ]
    }
   ],
   "source": [
    "result = [a[0] for a in result]\n",
    "print('\\n'.join(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c95828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\HCMUS_ComputerScience\\ComputationalThinking\\TeamProject\\reactProject\\CLIP_Model\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e8fc7d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'xx_ent_wiki_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m nlp = \u001b[43mspacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mxx_ent_wiki_sm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m      3\u001b[39m doc = nlp(\u001b[33m\"\u001b[39m\u001b[33mMr. John went to Starbucks Riverside.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc.ents:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HCMUS_ComputerScience\\ComputationalThinking\\TeamProject\\reactProject\\CLIP_Model\\venv\\Lib\\site-packages\\spacy\\__init__.py:52\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     29\u001b[39m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m     30\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] = util.SimpleFrozenDict(),\n\u001b[32m     36\u001b[39m ) -> Language:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m \u001b[33;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\HCMUS_ComputerScience\\ComputationalThinking\\TeamProject\\reactProject\\CLIP_Model\\venv\\Lib\\site-packages\\spacy\\util.py:531\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(name, vocab, disable, enable, exclude, config)\u001b[39m\n\u001b[32m    529\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[32m    530\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E941.format(name=name, full=OLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors.E050.format(name=name))\n",
      "\u001b[31mOSError\u001b[39m: [E050] Can't find model 'xx_ent_wiki_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\") \n",
    "doc = nlp(\"Mr. John went to Starbucks Riverside.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d658e9f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
